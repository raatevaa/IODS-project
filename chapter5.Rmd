---
title: "Chapter 5"
output: html_document
---

# Dimensionality reduction techniques

Statistical problems are often multidimensional: in order to understand different phenomena, it is necessary to "chop" variables into smaller pieces. This is what dimensionality reduction techniques aims for - we take original data variables and transform them into few components that collect together as much variance as possible from the original data. Thus, components make data interpretation, plotting etc. easier and more informative.


<br>

## Data overview

<br>

```{r human}
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt",sep  =",", header = T)

dim(human)
head(human)
str(human)

```

<br>

**Human -data** consists of 9 attributes and 195 observations (countries). Varibales describe countries "Human development index": The Human Development Index (HDI) is a summary measure of average achievement in key dimensions of human development: a long and healthy life, being knowledgeable and have a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions. (http://hdr.undp.org)

<br>

### Variables

- Edu2.FM (Education index (females))

- Labo.FM (Labour force participation rate (females))

- Edu.Exp (Expected years at schooling)

- Life.Exp (Life expectancy index)

- GNI (Gross national income ($))

- Mat.Mor (Material morality)

- Ado.Birth (Adolescent birth rate)

- Parli.F (Parliamentary representation (females))


<br>

### Data visualization

<br>

```{r human1, fig.height = 10, fig.width = 10, fig.align = "center"}
library("GGally")
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt",sep  =",", header = T)
human = transform(human, GNI = as.numeric(GNI))
human = transform(human, Mat.Mor = as.numeric(Mat.Mor))


lowerFn <- function(data, mapping, ...) {
  p <- ggplot(data = data, mapping = mapping) +
    geom_point(color = 'blue', alpha=0.3, size=1) +
    geom_smooth(color = 'black', method='lm', size=1,...)
  p
}

g <- ggpairs( 
  data = human,
  lower = list(
    continuous =  wrap(lowerFn) #wrap("smooth", alpha = 0.3, color = "blue", lwd=1) 
  ),
  upper = list(continuous = wrap("cor", size = 4))
)
g <- g + theme(
  axis.text = element_text(size = 4),
  axis.title = element_text(size = 4),
  legend.background = element_rect(fill = "white"),
  panel.grid.major = element_line(colour = NA),
  panel.grid.minor = element_blank(),
  panel.background = element_rect(fill = "grey95")
)
print(g, bottomHeightProportion = 0.5, leftWidthProportion = .5)
```

<br>

### Data interpretation

**Correlations:**

For the sake of clarity I chose one attribute which clearly correlates with other data variables and therefore explains the data well: Ado.Birth

**The adolescent birth rate** is the annual number of live births to adolescent women per 1,000 adolescent women. The adolescent birth rate is also referred to as the age-specific fertility rate for women aged 15-19


- Edu2.FM: Strong negative correlation (-0.53) - countries, where women have children at very young age do not participate in education process (at all).

- Edu.exp: Strong negative correlation (-0.70) - the expected years of schooling is very low if there is high adolescent birth rate. Children (males, females) are not participating in education process, because they reproduce quickly after puberty.

- Life.exp: Strong negative correlation (-0.73) - expected individual lifespan decreases as adolescent birth rate increases. There are many issues related - having children without stable incomes and education does not predict good starting point for offspring.

- GNI: trong negative correlation (-0.56) - GNI (country incomes) decrease while Ado.Birth increases: no education - no incomes. 

- Mat.Mor (def. The people of a society share a standard dignity through high morals making the moral fabric a keystone; keeping the arch of society and the morals it holds high-together. https://www.collinsdictionary.com/submission/8889/Moral+fabric) - positive correlation, adolescent birth rate increases Mat.Mor (does not make sense, I guess we should think that as Mat.Mor increases the moral of the country decreases)

<br>

## Principal component analysis (PCA) 

### 1. Unstandardized data

Next we will perform PCA-analysis for the HDI-dataset. We will be solving the components that capture most of the variability in the data.

First we perform PCA with unstandardized data.

```{r human2, fig.height = 10, fig.width = 10, fig.align = "center"}
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt",sep  =",", header = T)
human = transform(human, GNI = as.numeric(GNI))
human = transform(human, Mat.Mor = as.numeric(Mat.Mor))
pca_human <- prcomp(human, scale=FALSE)
print("The variability captured by the principal components:")
pca_human
biplot(pca_human)
```

<br>

### 2. Standardized data

Next we will run PCA and plot results with standardized data.

Definition of scale: "log simply takes the logarithm (base e, by default) of each element of the vector.
scale, with default settings, will calculate the **mean** and **standard deviation of the entire vector**, then "scale" each element by those values **by subtracting the mean and dividing by the sd**." (https://stackoverflow.com/questions/20256028/understanding-scale-in-r)

<br>

```{r human3, fig.height = 10, fig.width = 10, fig.align = "center"}
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt",sep  =",", header = T)
human = transform(human, GNI = as.numeric(GNI))
human = transform(human, Mat.Mor = as.numeric(Mat.Mor))
human_std <- scale(human)
pca_human <- prcomp(human_std, scale=T)
print("The variability captured by the principal components:")
pca_human
biplot(pca_human)


```

<br>

### PCA - Conclusion

As we can recognize from figures above, PCA-analysis on unstandardized data can produce unwanted results - mainly because of variables with the highest sample variances tend to be emphasized in the first few principal components. Thus, principal component analysis using the covariance function should only be considered if all of the variables have the same units of measurement. 

- Results on PCA-analysis depend on measurement scales: it is recommended to standardize data before analysis. In our case, GNI (country income, values in thousands) over-emphasizes the data.

<br>

## PCA -visualization continued

```{r, human4, fig.height = 10, fig.width = 10, fig.align = "center"}
library(dplyr); library(factoextra)
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt",sep  =",", header = T)
human = transform(human, GNI = as.numeric(GNI))
human = transform(human, Mat.Mor = as.numeric(Mat.Mor))
human_std <- scale(human)
pca_human <- prcomp(human_std, scale=T)
fviz_eig(pca_human)
s <- summary(pca_human)
print("Summary of PCA-analysis")
s
pca_pr <- round(1*s$importance[2, ], digits = 2)
print("The variability captured by the principal components:")
pca_pr
#str(as.data.frame(as.table(human_std)))
fviz_pca_var(pca_human, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping
             )
# Contributions of variables to PC1
fviz_contrib(pca_human, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(pca_human, choice = "var", axes = 2, top = 10)
pca_pr = pca_pr*100
pc_lab = paste0(names(pca_pr), " (", pca_pr, "%)")
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "red"), xlab = pc_lab[1], ylab = pc_lab[2])
```

<br>

From figure above, first principal component (PC1) explains 54 % of the variance in data. Second component (PC2) explains much less, total 16 %.

### Personal interpretation of 2 principal component dimensions

As we can see from the biplot above, it seems that data can be divided into three variable groups, which are:

1. Edu.Exp, GNI, Edu2.FM, LIfe.exp

2. Parli.F, Labo.FM

3. Mat.Mor, Abo.Birth

If we have to choose two data dimensions (variables) to explain the data, I would choose Life.Exp and Mat.Mor, since they have the largest contribution in 1st. principal component (bar-chart above). Although, I believe second dimension could be taken from second component, such as Labo.FM, which contributes over 50 %.

<br>

## Exercise with Tea-dataset

### Data summary

```{r, tea}
library(FactoMineR);library(dplyr);library(MASS)
data(tea)
keep <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- dplyr::select(tea, keep)
str(tea_time)
summary(tea_time)
```


<br>

Tea-data has 300 observations and originally 36 variables; we chose 6 variables for further examination. Every attribute is categorical: 2 are binary others have 3-4 classes.

<br>

### Data visualization 

```{r, tea1, fig.height = 10, fig.width = 10, fig.align = "center"}
library(FactoMineR);library(MASS);library(dplyr);library(tidyr);library(ggplot2)
data(tea)
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- dplyr::select(tea, keep_columns)
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
```


<br>

### Multiple correspondence analysis


```{r, tea2, fig.height = 10, fig.width = 10, fig.align = "center"}
library(FactoMineR);library(MASS);library(dplyr);library(tidyr);library(ggplot2);library("factoextra")
data(tea)
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- dplyr::select(tea, keep_columns)
mca <- MCA(tea_time, graph = FALSE)
summary(mca)
#
fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 45))
fviz_contrib(mca, choice = "var", axes = 1, top = 10)
fviz_contrib(mca, choice = "var", axes = 2, top = 10)
plot(mca, invisible=c("ind"))
plot(mca, invisible=c("ind"),habillage = "quali")
fviz_mca_var(mca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, # Avoid text overlapping
             ggtheme = theme_minimal())
fviz_mca_ind(mca, 
             label = "none", # hide individual labels
             habillage = "how", # color by groups 
             palette = c("#00AFBB", "#E7B800","#d9b3ff"),
             addEllipses = TRUE, ellipse.type = "confidence",
             ggtheme = theme_minimal())
```

<br>

## MCA-analysis conclusion

<br>

Conclusion of the MCA-analysis on tea-data is that data can be described very poorly by any individual dimension (categorical class), as we can see from the first bar-plot: all the variables are somewhat equal - from 15 % to 5 %. In the first dimensions, most of the data variance is caught by attributes "where" and "how". Both explain variance over 20 %. 

In Variable-categories figure, we are able to recognize that how- and where -related var. are well represented by our 2 dimensions (MCA). Cos2-value defines how well category can be explained by components produced by MCA.

In the last figure, I plotted the observations in MCA-dimensions and categorized them with "how"- variable: it seems the data can be described fairly well with those 3 categories. Same phenomenom can be seen with "where"-variable.

<br>



